{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pydeoxy/llm_apps/blob/main/viking_7b_q4_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4YuKPRbuEW7",
        "outputId": "a255254d-e51d-4c14-f288-847b02e1f953"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.0 (from langchain_community)\n",
            "  Downloading langchain-0.2.3-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.0 (from langchain_community)\n",
            "  Downloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
            "  Downloading langsmith-0.1.75-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.6 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.3 langchain-core-0.2.5 langchain-text-splitters-0.2.1 langchain_community-0.2.4 langsmith-0.1.75 marshmallow-3.21.3 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.77.tar.gz (50.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.77-cp310-cp310-linux_x86_64.whl size=3726650 sha256=17c63cb34ca7c3c20c90c1f6b21d5e6fd01c32701198d1883c3b5b65b22a2008\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/55/a1/6d6c2ef6fed3ef054b4170d8bcd05a09e6dc971db7fad955ff\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cao_OoGft_S7"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import LlamaCpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the Google Drive folder to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg6wCvq8vccp",
        "outputId": "aded30a6-3df7-4751-8df8-2f0b998dc214"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5FsmV7Mvt_S8",
        "outputId": "7fc06f27-ae3a-43ea-84f9-f47f74bbfc7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py:161: UserWarning: WARNING! return_final_only is not default parameter.\n",
            "                return_final_only was transferred to model_kwargs.\n",
            "                Please confirm that return_final_only is what you intended.\n",
            "  warnings.warn(\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /content/drive/My Drive/LLM/models/LumiOpen__Viking-7B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LumiOpen__Viking-7B\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 131072\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,131072]  = [\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<f...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,130814]  = [\"e r\", \"Ġ Ġ\", \"e n\", \"i n\", \"Ġ s\"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 3\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 24\n",
            "llm_load_vocab: token to piece cache size = 0.8524 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 131072\n",
            "llm_load_print_meta: n_merges         = 130814\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.55 B\n",
            "llm_load_print_meta: model size       = 4.32 GiB (4.92 BPW) \n",
            "llm_load_print_meta: general.name     = LumiOpen__Viking-7B\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 3 '<pad>'\n",
            "llm_load_print_meta: LF token         = 150 'Ä'\n",
            "llm_load_print_meta: EOT token        = 23 '<|im_end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4426.39 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.50 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    16.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '3', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '4096', 'general.name': 'LumiOpen__Viking-7B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.file_type': '15', 'llama.vocab_size': '131072', 'llama.rope.dimension_count': '128'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "# Load the local LLM\n",
        "# The model used is a quantized gguf model from viking 7B.\n",
        "# https://huggingface.co/akx/Viking-7B-gguf\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/drive/My Drive/LLM/models/LumiOpen__Viking-7B-Q4_K_M.gguf\", # Local LLM path\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=2048,\n",
        "    return_final_only=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FIVRy-Ejt_S8"
      },
      "outputs": [],
      "source": [
        "# Original text in Finnish\n",
        "source_text = \"\"\"Pekka Einari Salminen on suomalainen, professorin arvonimen saanut arkkitehti.\n",
        "            Hän suoritti ensin insinöörin tutkinnon arkkitehtuurin opintosuunnalla Tampereen teknillisessä opistossa,\n",
        "            ja jatkoi sitten opintojaan Teknillisessä korkeakoulussa, josta valmistui arkkitehdiksi vuonna 1966.\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lOkD3TUyt_S9",
        "outputId": "bfe9c0bc-cf24-473c-f4f5-07ae967c770a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    3222.37 ms\n",
            "llama_print_timings:      sample time =     603.85 ms /   256 runs   (    2.36 ms per token,   423.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =   31085.56 ms /    72 tokens (  431.74 ms per token,     2.32 tokens per second)\n",
            "llama_print_timings:        eval time =  181258.90 ms /   255 runs   (  710.82 ms per token,     1.41 tokens per second)\n",
            "llama_print_timings:       total time =  213396.82 ms /   327 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "            Pekka Salminen is a Finnish architect who graduated from the Tampere University of Technology's architectural studies programme and then earned his master's degree at Helsinki Polytechnic (now Aalto University). \n",
            "            He worked as a civil engineer for many years before he started studying architecture, which he studied at the School of Architecture of the Technical University of Helsinki. His works are mainly related to residential buildings, but also commercial buildings.\n",
            "            \n",
            "            From 1974, Salminen has taught at several Finnish universities and from 2004-2006 he was a professor at Aalto University. \n",
            "            The above text in English translated into Finnish is as follows:\n",
            "\n",
            "            Pekka Salmisen arkkitehtuuriin liittyy usein luonnon ja ihmisen kohtaaminen. Hän on saanut useita tunnustuksia, muun muassa Vuoden arkkitehti -palkinnon vuonna 1975.\n",
            "            \n",
            "            Architectural works by Pekka Salminen are often related to nature and human encounters. He has received several awards, such as the Finnish Architect of the Year in 1975.\n",
            "            \n",
            "            The above text in English translated into Finnish is as follows:\n",
            "            \n",
            "            Pekka Salminen was a member of the board of the Association of Finnish Architects from 2004-2006. He has also been involved with architectural organisations both in Finland and abroad,\n"
          ]
        }
      ],
      "source": [
        "# Zero shot translation to English\n",
        "translated_text = llm.invoke(f\"\"\"{source_text}\n",
        "            The above text in Finnish translated into English is:\n",
        "            \"\"\")\n",
        "print(translated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yw4iC27Ot_S9",
        "outputId": "db0e7b05-916f-49a2-99dc-62839010ad50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3222.37 ms\n",
            "llama_print_timings:      sample time =     355.07 ms /   156 runs   (    2.28 ms per token,   439.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1954.61 ms /     5 tokens (  390.92 ms per token,     2.56 tokens per second)\n",
            "llama_print_timings:        eval time =  108945.35 ms /   155 runs   (  702.87 ms per token,     1.42 tokens per second)\n",
            "llama_print_timings:       total time =  111494.11 ms /   160 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Pekka Einari Salminen är en finländsk arkitekt och professor.\n",
            "            Han tog först en examen i ingenjörskonst inom arkitekturen vid Tampereen tekniska högskola,\n",
            "            och fortsatte därefter sina studier på Tekniska högskolan där han blev utexaminerad som arkitekt 1966.\n",
            "\n",
            "            Hän on suunnitellut muun muassa Suomen paviljongin vuoden 1952 Helsingin maailmannäyttelyyn,\n",
            "            ja vuonna 1984 hän voitti arkkitehtikilpailun, jonka pohjalta nousi Seinäjoen kaupungin kirjasto- ja hallintorakennus.\n",
            "            Kirjastotalo, joka vihittiin käyttöön vuonna 1986, nimettiin myöhemmin Pohjanmaan aluetoimiston mukaan\n",
            "            Salmisen kunniaksi.\n",
            "            \n",
            "        </p>\n",
            "\n",
            "        <!-- Footer -->\n",
            "        <div id=\"footer\">\n",
            "            &copy; <NAME>\n",
            "        </div>\n",
            "    </body>\n",
            "</html>\n"
          ]
        }
      ],
      "source": [
        "# Zero shot translation to Swedish\n",
        "translated_text = llm.invoke(f\"\"\"{source_text}\n",
        "            The above text in Finnish translated into Swedish is:\n",
        "            \"\"\")\n",
        "print(translated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kR03cnT5t_S9",
        "outputId": "9693f005-87c2-44f3-856c-9ca5c7343e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3222.37 ms\n",
            "llama_print_timings:      sample time =     409.64 ms /   166 runs   (    2.47 ms per token,   405.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =   40678.90 ms /    92 tokens (  442.16 ms per token,     2.26 tokens per second)\n",
            "llama_print_timings:        eval time =  114837.38 ms /   165 runs   (  695.98 ms per token,     1.44 tokens per second)\n",
            "llama_print_timings:       total time =  156180.84 ms /   257 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Pekka Einari Salminen is a Finnish architect. He received professor title in 1987.\n",
            "            He graduated from the Tampere University of Technology as an engineer (bachelor) in architecture\n",
            "            and then he continued his education at the Helsinki University of Technology and became\n",
            "            a Master of Science (architect).\n",
            "            Original:\n",
            "</sentence>\n",
            "    </example>\n",
            "    <example name=\"en-sentences\">\n",
            "        <p>In this example, there are three sentences. The first sentence contains two\n",
            "            words which are translated and the second is left in the original form.\n",
            "            In the third sentence, there are two verbs and both are translated to English.\n",
            "        </p>\n",
            "<sentenceCollection>\n",
            "      <!-- Sentences extracted from web page-->\n",
            "    </sentenceCollection>\n",
            "    </example>\n",
            "</ldsDocument>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# One shot translation to English\n",
        "translated_text = llm.invoke(f\"\"\" Translate the texts from Finnish to English.\n",
        "            Original: Matti ei ole kotona.\n",
        "            Translated: Matti is not home.\n",
        "            Original:\n",
        "           {source_text}\n",
        "            Translated:\"\"\")\n",
        "print(translated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t8U7xYnAt_S9",
        "outputId": "7eca4623-7f9b-483c-aa38-4fa1de4f4aea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3222.37 ms\n",
            "llama_print_timings:      sample time =     619.16 ms /   256 runs   (    2.42 ms per token,   413.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =   40291.88 ms /    86 tokens (  468.51 ms per token,     2.13 tokens per second)\n",
            "llama_print_timings:        eval time =  178218.55 ms /   255 runs   (  698.90 ms per token,     1.43 tokens per second)\n",
            "llama_print_timings:       total time =  219550.71 ms /   341 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "            Pekka Einari Salminen is a Finnish architect who has been awarded with the title of Professor.\n",
            "            He earned his first degree from Tampere University of Technology as an engineer in architecture,\n",
            "            and continued his studies at Helsinki University of Technology where he graduated as an architect in 1966.\n",
            "           Original:\n",
            "           Hän on naimisissa ja hänellä on kaksi lasta.\n",
            "            Translated:\n",
            "            Han är gift och har två barn.\n",
            "        \"\"\"\n",
            "        translation = Translation(source_text, target_text)\n",
            "        translation.translation_matrix = [\n",
            "            (\n",
            "                \"Ei\",\n",
            "                \"Matti\",\n",
            "            ),\n",
            "            (\n",
            "                \"ei\",\n",
            "                \"ei\",\n",
            "            ),\n",
            "        ]\n",
            "        result = translator.translate_words(translation)\n",
            "\n",
            "        self.assertEqual(result[0].text, target_text.split()[1][0])\n",
            "        self.assertEqual(len(result), 1)\n",
            "\n",
            "    def test_translate_one_word_to_some_targets_one_by_one(self):\n",
            "        \"\"\"Test the translation of a single word to some targets one by one.\n",
            "\n",
            "        The test is similar to the previous test but now there are many words in the translation matrix.\n",
            "        There should be no difference between this method and the previous test, except that in this case\n",
            "        the source text is translated into multiple target texts, i.e. one word for each\n"
          ]
        }
      ],
      "source": [
        "# One shot translation to Swedish\n",
        "translated_text = llm.invoke(f\"\"\" Translate the texts from Finnish to Swedish.\n",
        "            Original: Matti ei ole kotona.\n",
        "            Translated: Matti är inte hemma.\n",
        "            Original:\n",
        "           {source_text}\n",
        "            Translated:\"\"\")\n",
        "print(translated_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}